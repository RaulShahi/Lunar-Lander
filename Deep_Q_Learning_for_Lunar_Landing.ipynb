{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RaulShahi/Lunar-Lander/blob/main/Deep_Q_Learning_for_Lunar_Landing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbZcI9ZXHl3a"
      },
      "source": [
        "# Deep Q-Learning for Lunar Landing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8yPRjteXgPb"
      },
      "source": [
        "## Part 0 - Installing the required packages and importing the libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slEm5teGWjWU"
      },
      "source": [
        "### Installing Gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbnq3XpoKa_7"
      },
      "outputs": [],
      "source": [
        "!pip install gymnasium\n",
        "!pip install \"gymnasium[atari, accept-rom-license]\"\n",
        "!apt-get install -y swig\n",
        "!pip install gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brqiMN3UW9T9"
      },
      "source": [
        "### Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZaKXP_aMl9O"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd\n",
        "from torch.autograd import Variable\n",
        "from collections import deque, namedtuple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzlDKXvkXzGI"
      },
      "source": [
        "## Part 1 - Building the AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtG6Zc83YYy3"
      },
      "source": [
        "### Creating the architecture of the Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(nn.Module):\n",
        "  def __init__(self, state_size, action_size, seed=42) -> None:\n",
        "    super(Network, self).__init__()\n",
        "    self.seed = torch.manual_seed(seed)\n",
        "    self.state_size = state_size\n",
        "    self.action_size = action_size\n",
        "    self.fc1 = nn.Linear(state_size, 64)\n",
        "    self.fc2 = nn.Linear(64, 64)\n",
        "    self.fc3 = nn.Linear(64, action_size)\n",
        "\n",
        "  def forward(self, state):\n",
        "    x = self.fc1(state)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    x = F.relu(x)\n",
        "    return self.fc3(x)\n"
      ],
      "metadata": {
        "id": "DZ-ThcbzR54i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxVrBnFWZKb1"
      },
      "source": [
        "## Part 2 - Training the AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T364fz9qZb2j"
      },
      "source": [
        "### Setting up the environment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "env = gym.make('LunarLander-v3') # The Lunar Lander environment was upgraded to v3\n",
        "state_shape = env.observation_space.shape\n",
        "state_size = env.observation_space.shape[0]\n",
        "number_actions = env.action_space.n\n",
        "print('State shape: ', state_shape)\n",
        "print('State size: ', state_size)\n",
        "print('Number of actions: ', number_actions)"
      ],
      "metadata": {
        "id": "41sgnpCwtd5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_dZmOIvZgj-"
      },
      "source": [
        "### Initializing the hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 5e-4\n",
        "minibatch_size = 100\n",
        "discount_factor = 0.99\n",
        "replay_buffer_size = int(1e5) #100000\n",
        "interpolation_parameter = 0.001 #for soft updates"
      ],
      "metadata": {
        "id": "iKtwXwYOXbCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hD_Vs-bYnip"
      },
      "source": [
        "### Implementing Experience Replay"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayMemory(object):\n",
        "  def __init__(self, capacity) -> None:\n",
        "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.capacity = capacity\n",
        "    self.memory = []\n",
        "\n",
        "  def push(self, event):\n",
        "    self.memory.append(event)\n",
        "    if len(self.memory) > self.capacity:\n",
        "      del self.memory[0]\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    experiences = random.sample(self.memory, k=batch_size)\n",
        "    #states = np.vstack([e[0] for e in experiences if e is not None]) #getting all the states for all the sampled experiences\n",
        "    #we need to convert this stack of states into pytorch tensors for training and backpropagation\n",
        "\n",
        "    states = torch.from_numpy(np.vstack([e[0] for e in experiences if e is not None])).float().to(self.device)\n",
        "    actions = torch.from_numpy(np.vstack([e[1] for e in experiences if e is not None])).long().to(self.device)\n",
        "    rewards = torch.from_numpy(np.vstack([e[2] for e in experiences if e is not None])).float().to(self.device)\n",
        "    next_states = torch.from_numpy(np.vstack([e[3] for e in experiences if e is not None])).float().to(self.device)\n",
        "    dones = torch.from_numpy(np.vstack([e[4] for e in experiences if e is not None]).astype(np.uint8)).float().to(self.device) #np.uint8 => boolean\n",
        "    return states, next_states, actions, rewards, dones\n"
      ],
      "metadata": {
        "id": "LU610WT8YKwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmEkbFbUY6Jt"
      },
      "source": [
        "### Implementing the DQN class"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent():\n",
        "\n",
        "  def __init__(self, state_size, action_size):\n",
        "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.state_size = state_size\n",
        "    self.action_size = action_size\n",
        "    self.local_qnetwork = Network(state_size, action_size).to(self.device)\n",
        "    self.target_qnetwork = Network(state_size, action_size).to(self.device)\n",
        "    self.optimizer = optim.Adam(self.local_qnetwork.parameters(), lr = learning_rate)\n",
        "    self.memory = ReplayMemory(replay_buffer_size)\n",
        "    self.t_step = 0\n",
        "\n",
        "  def step(self, state, action, reward, next_state, done):\n",
        "    self.memory.push((state, action, reward, next_state, done))\n",
        "    self.t_step = (self.t_step + 1) % 4\n",
        "    if self.t_step == 0:\n",
        "      if len(self.memory.memory) > minibatch_size:\n",
        "        experiences = self.memory.sample(100)\n",
        "        self.learn(experiences, discount_factor)\n",
        "\n",
        "  def act(self, state, epsilon = 0.):\n",
        "    state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
        "    self.local_qnetwork.eval()\n",
        "    with torch.no_grad():\n",
        "      action_values = self.local_qnetwork(state)\n",
        "    self.local_qnetwork.train()\n",
        "    if random.random() > epsilon:\n",
        "      return np.argmax(action_values.cpu().data.numpy())\n",
        "    else:\n",
        "      return random.choice(np.arange(self.action_size))\n",
        "\n",
        "  def learn(self, experiences, discount_factor):\n",
        "    states, next_states, actions, rewards, dones = experiences\n",
        "    next_q_targets = self.target_qnetwork(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "    q_targets = rewards + discount_factor * next_q_targets * (1 - dones)\n",
        "    q_expected = self.local_qnetwork(states).gather(1, actions)\n",
        "    loss = F.mse_loss(q_expected, q_targets)\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "    self.soft_update(self.local_qnetwork, self.target_qnetwork, interpolation_parameter)\n",
        "\n",
        "  def soft_update(self, local_model, target_model, interpolation_parameter):\n",
        "    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "      target_param.data.copy_(interpolation_parameter * local_param.data + (1.0 - interpolation_parameter) * target_param.data)"
      ],
      "metadata": {
        "id": "k9N5JYoJxb5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1tZElccZmf6"
      },
      "source": [
        "### Initializing the DQN agent"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent(state_size, number_actions)"
      ],
      "metadata": {
        "id": "yMahZJnt0EgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8v0PtUfaVQp"
      },
      "source": [
        "### Training the DQN agent"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "number_of_episodes = 2000\n",
        "maximum_number_timesteps_per_episode = 1000\n",
        "epsilon_starting_value = 1.0\n",
        "epsilon_ending_value = 0.01\n",
        "epsilon_decay_value = 0.995\n",
        "epsilon = epsilon_starting_value\n",
        "scores_on_100_episodes = deque(maxlen= 100)\n",
        "\n",
        "for episode in range(1, number_of_episodes + 1):\n",
        "  state, _ = env.reset()\n",
        "  score = 0\n",
        "\n",
        "  for t in range(0, maximum_number_timesteps_per_episode):\n",
        "    action = agent.act(state, epsilon)\n",
        "    #after taking the action, we end up in a new state and get a reward\n",
        "    next_state, reward, done, _, _ = env.step(action)\n",
        "    agent.step(state, action, reward, next_state, done)\n",
        "    state = next_state\n",
        "    score += reward\n",
        "    if done:\n",
        "      break\n",
        "  scores_on_100_episodes.append(score)\n",
        "  epsilon = max(epsilon_ending_value, epsilon_decay_value*epsilon)\n",
        "  print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_on_100_episodes)), end = \"\")\n",
        "  if episode % 100 == 0:\n",
        "    print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_on_100_episodes)))\n",
        "  if np.mean(scores_on_100_episodes) >= 200.0:\n",
        "    print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(episode - 100, np.mean(scores_on_100_episodes)))\n",
        "    torch.save(agent.local_qnetwork.state_dict(), 'checkpoint.pth')\n",
        "    break\n"
      ],
      "metadata": {
        "id": "JGeoBukl1cB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8CNwdOTcCoP"
      },
      "source": [
        "## Part 3 - Visualizing the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cb9nVvU2Okhk"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import io\n",
        "import base64\n",
        "import imageio\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def show_video_of_model(agent, env_name):\n",
        "    env = gym.make(env_name, render_mode='rgb_array')\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    frames = []\n",
        "    while not done:\n",
        "        frame = env.render()\n",
        "        frames.append(frame)\n",
        "        action = agent.act(state)\n",
        "        state, reward, done, _, _ = env.step(action.item())\n",
        "    env.close()\n",
        "    imageio.mimsave('video.mp4', frames, fps=30)\n",
        "\n",
        "show_video_of_model(agent, 'LunarLander-v3')\n",
        "\n",
        "def show_video():\n",
        "    mp4list = glob.glob('*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else:\n",
        "        print(\"Could not find video\")\n",
        "\n",
        "show_video()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "class Agent():\n",
        "  #1. Initialization\n",
        "  def __init__(self,state_size, action_size):\n",
        "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.state_size = state_size\n",
        "    self.action_size = action_size\n",
        "    self.local_qnetwork = Network(state_size, action_size).to(self.device) #move to designated device\n",
        "    self.target_qnetwork = Network(state_size, action_size).to(self.device) #move to designated device\n",
        "    self.optimizer = optim.Adam(self.local_qnetwork.parameters(), lr= learning_rate)\n",
        "    self.replay_memory = ReplayMemory(capacity=replay_buffer_size)\n",
        "    self.t_step = 0\n",
        "\n",
        "  #2. Now store the experiences and learn from them\n",
        "  def step(self, state, action, rewards, next_state, done):\n",
        "    #a. store the experience\n",
        "    self.replay_memory.push((self, action, rewards, next_state, done))\n",
        "\n",
        "    #bwhen to learn from them? increment the t_step counter and reset every certain step\n",
        "    self.t_step = (self.t_step + 1) % 4\n",
        "    if self.t_step == 0:\n",
        "      #we dont learn from one experience only, but on the minibatch of experiences\n",
        "      #so check the number of experiences in the replay memory is greater than initialized batchsize\n",
        "\n",
        "      if len(self.replay_memory.memory) > minibatch_size:\n",
        "        experiences = self.replay_memory.sample(100)\n",
        "        self.learn(experiences, discount_factor)\n",
        "\n",
        "  #3.select an action based on given state and a certain epsilon value\n",
        "  def act(self, state, epsilon=0.):\n",
        "    \"\"\"a. at this point, the state is a numpy array and we need to make sure it is a torch tensor. so convert\n",
        "    very important to add an extra dimension to the state tensor to represent batch size.\n",
        "    This batch dimension is added at the beginning(index 0) using the unsqueeze(0) method.\n",
        "    This is important because the network expects batched inputs, even if the batch size is one.\"\"\"\n",
        "    state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
        "\n",
        "    \"\"\"\n",
        "    b. Setting the local q network to evaluation mode\n",
        "    Before forwarding the state through the local Q network to obtain action values,\n",
        "    we set the network to evaluation mode by calling its eval() method.\n",
        "    This disables certain layers like dropout and batch normalization, ensuring consistent inference behavior.\n",
        "    \"\"\"\n",
        "    self.local_qnetwork.eval()\n",
        "\n",
        "    \"\"\"\n",
        "    c. Forward Pass with No Gradient Computation\n",
        "    To perform inference without tracking gradients, we use PyTorch's torch.no_grad() context manager.\n",
        "    Inside this block, we forward the state through the local Q network to obtain the action values (Q-values) for each possible action.\n",
        "    \"\"\"\n",
        "\n",
        "    with torch.no_grad():\n",
        "      action_values = self.local_qnetwork(state)#forward pass\n",
        "\n",
        "    \"\"\"\n",
        "    d. Returning to Training Mode\n",
        "    After inference, we set the local Q network back to training mode by calling its train() method.\n",
        "    This re-enables layers like dropout and batch normalization for training.\n",
        "    \"\"\"\n",
        "    self.local_qnetwork.train()\n",
        "\n",
        "    \"\"\"\n",
        "    e. Epsilon-Greedy Action Selection Policy\n",
        "    The epsilon-greedy policy works as follows:\n",
        "\n",
        "    Generate a random number between 0 and 1.\n",
        "    If this random number is greater than epsilon, select the action with the highest Q-value.\n",
        "    Otherwise, select a random action.\n",
        "    This balances exploitation (choosing the best known action) and exploration (trying random actions).\n",
        "    \"\"\"\n",
        "    if random.random() > epsilon:\n",
        "      return np.argmax(action_values.cpu().data.numpy()) #.cpu() means sending to cpu, argmax expects numpy format, so data.numpy()\n",
        "    else:\n",
        "      return random.choice(np.arange(self.action_size))\n",
        "\n",
        "#4. Update the agent's Q values based on sample experiences\n",
        "def learn(self, experiences, discount_factor):\n",
        "  \"\"\"\n",
        "  a. The first step is to unpack the sampled experiences into their respective components: states, next states, actions, rewards, and dones. These are extracted from the experience tuples.\n",
        "  \"\"\"\n",
        "  states, next_states, actions, rewards, dones = experiences\n",
        "  \"\"\"\n",
        "  b. Computing Maximum Predicted Q Values for Next States\n",
        "  We obtain the maximum predicted Q values for the next states from the target network. This is necessary to compute the Q targets for the current states.\n",
        "\n",
        "  We forward propagate the next states through the target Q network to get the action values.\n",
        "  Then, we detach the resulting tensor from the computation graph to avoid tracking gradients during backpropagation.\n",
        "\n",
        "  Next, we use the max function along dimension one (the action dimension) to get the maximum values.\n",
        "  Since max returns both the maximum values and their indices, we select only the maximum values tensor by indexing with zero.\n",
        "\n",
        "  Finally, we add a dimension back at position one using .unsqueeze(1) to maintain the batch dimension.\n",
        "  \"\"\"\n",
        "\n",
        "  next_q_targets = self.target_qnetwork(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "\n",
        "  \"\"\"\n",
        "  c. Calculating Q Targets for Current States\n",
        "  The Q targets for the current states are computed using the formula:\n",
        "  Q targets=rewards+γ×next Q targets×(1−dones)\n",
        "  Here,\n",
        "  γ\n",
        "  γ is the discount factor, and\n",
        "  dones indicates whether the episode has ended.\n",
        "  \"\"\"\n",
        "  q_targets = rewards + (discount_factor * next_q_targets * (1-dones))\n",
        "\n",
        "  \"\"\"d. Getting Expected Q Values from Local Q Network\n",
        "  We forward propagate the current states through the local Q network to obtain the expected Q values.\n",
        "  Then, we gather the Q values corresponding to the taken actions using the gather function along dimension one.\n",
        "  \"\"\"\n",
        "  q_expected = self.local_qnetwork(states).gather(1, actions)\n",
        "\n",
        "  \"\"\"e. Computing the Loss\n",
        "  We compute the loss between the expected Q values and the target Q values using the mean squared error (MSE) loss function from the functional module F.\n",
        "  \"\"\"\n",
        "  loss = F.mse_loss(q_expected, q_targets)\n",
        "\n",
        "  \"\"\"\n",
        "  f.Backpropagation and Optimization\n",
        "  Before backpropagation, we reset the gradients of the optimizer by calling its zero_grad() method.\n",
        "  Then, we backpropagate the loss to compute the gradients with respect to the model parameters.\n",
        "  \"\"\"\n",
        "  self.optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "\n",
        "  \"\"\"Performing an Optimization Step\n",
        "  We perform a single optimization step to update the model parameters by calling the step() method of the optimizer.\n",
        "  \"\"\"\n",
        "  self.optimizer.step()\n",
        "  \"\"\"\n",
        "  Updating Target Network Parameters\n",
        "  Finally, we update the target network's parameters with those of the local network using a soft update method.\n",
        "  \"\"\"\n",
        "  self.soft_update(self.local_qnetwork, self.target_qnetwork, interpolation_parameter)\n",
        "\n",
        "#5. Update the target network's parameters\n",
        "def soft_update(self, local_model, target_model, interpolation_parameter):\n",
        "  for target_param, local_param in zip(target_model.parameters(), local_model.paramters()):\n",
        "    target_param.data.copy_(interpolation_parameter * local_param.data + (1.0 - interpolation_parameter)* target_param.data)"
      ],
      "metadata": {
        "id": "t90jk1uA1GtV"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}